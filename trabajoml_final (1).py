# -*- coding: utf-8 -*-
"""TrabajoML_FINAL.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kbLh9AFQymld56mLameIqoNoUlupNJZM

# **Trabajo Final**

Manuela Gómez y Alexandra Vasco
"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder, OrdinalEncoder, StandardScaler
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

import threading
import time

# Función para mantener activa la conexión
def mantener_conexion_activa():
    while True:
        time.sleep(60)
        print("Manteniendo la conexión activa...")

# Iniciar el hilo en segundo plano
thread = threading.Thread(target=mantener_conexion_activa)
thread.daemon = True  # Permite que el hilo se cierre cuando se detenga el entorno
thread.start()

"""# 1) Carga de datos (puede ocultarse)"""

data=pd.read_csv(r'/content/Beneficiarios_M_s_Familias_en_Acci_n_20241022.csv',usecols=range(22))
data = data[['Bancarizado', 'RangoEdad', 'TipoPoblacion']]
data= data[data['Bancarizado'] != 'ND']
data = data.dropna()
moda_tipopoblacion = data['TipoPoblacion'].mode()[0]
data['TipoPoblacion'].replace(['ND'], moda_tipopoblacion, inplace=True)
label_encoder = LabelEncoder()
data['Bancarizado'] = label_encoder.fit_transform(data['Bancarizado'])
ordinal_encoder = OrdinalEncoder(categories=[['06-17', '18-29', '30-49', '50-65', '>65']])
data['RangoEdad'] = ordinal_encoder.fit_transform(data[['RangoEdad']])
data = pd.get_dummies(data, columns=['TipoPoblacion'], drop_first= False)

"""# 2) Construcción del modelo de clasificación con Redes Neuronales"""

# División de los datos en entrenamiento y prueba
X = data.drop(columns=['Bancarizado'])
y = data['Bancarizado']

# Dividir en 70% (train) y 20% (test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=13, stratify=y)

# Escalado de los datos
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# División del conjunto de test final en test real (80%) y datos no vistos (20%)
X_test_real, X_nuevos, y_test_real, y_nuevos = train_test_split(X_test, y_test, test_size=0.2, random_state=13, stratify=y_test)

len(X_train)

# Creación el modelo
mlp = MLPClassifier(random_state=42, verbose=True)

# Ajuste del modelo
mlp.fit(X_train, y_train)

"""# 3) Métricas de rendimiento por clase"""

# Predicciones
y_pred = mlp.predict(X_test_real)

# Reporte de clasificación detallado
print(classification_report(y_test_real, y_pred))

"""# 4)  Matriz de Confusión"""

# Matriz de confusión
conf_matrix = confusion_matrix(y_test_real, y_pred)

# Visualización de la matriz de confusión
sns.heatmap(conf_matrix, annot=True, cmap='Blues', fmt='g')
plt.xlabel('Predicción')
plt.ylabel('Real')
plt.title('Matriz de Confusión')
plt.show()

"""# 5) Seleccionar métrica


* Exactitud: Mide la proporción de predicciones correctas, pero requiere datos completamente balanceados

* Precisión: Es útil si se quiere minimizar los falsos positivos. (falsos 1)

* Sensibilidad (Recall): Es útil si se quiere minimizar los falsos negativos. (falsos 0)

* F1-Score: Como es una combinación de precisión y sensibilidad, el F1-Score asegura que no se priorice solo minimizar los falsos positivos o solo los falsos negativos.

En este caso, como es necesario tanto minimizar la cantidad de falsos 0 y falsos 1, se escoge **F1 score como la métrica**

# 6) Grid Search
"""

hiperparametros= {
    'solver': ['adam', 'sgd', 'lbfgs'],  # Algoritmo de solución
    'learning_rate_init': [0.001, 0.01, 0.1],  # Valores de tasa de aprendizaje
    'learning_rate': ['constant', 'adaptive'],  # tasa de aprendizaje
    'hidden_layer_sizes': [(10,), (50,), (100,), (50, 50)],  # Cantidad de capas y neuronas por capa
    'alpha': [0.01, 0.1, 0.2]  # Coeficiente de regularización
}

# Grid Search
grid_search = GridSearchCV( estimator = mlp, param_grid= hiperparametros, cv=3,n_jobs=-1, verbose=2,
    scoring='f1_macro'  # Usamos F1, ya que es la métrica seleccionada
)


grid_search.fit(X_train, y_train)

"""# 7) Mejores Hiperparametros"""

best_params = grid_search.best_params_
print("Mejores hiperparámetros:", best_params)

"""# 8) Parametros Red Neuronal





"""

mejor_mlp = grid_search.best_estimator_

# topología (capas y neuronas por capa)
print("Topología de la red (capas ocultas):", mejor_mlp.hidden_layer_sizes)

# Función de activación
print("Función de activación:", mejor_mlp.activation)

# Imprimir los pesos de las capas
for i, weights in enumerate(mejor_mlp.coefs_):
    print(f"Pesos de la capa {i+1}:\n", weights)

"""# 9) Predicciones con Datos No Vistos

"""

y_nuevos_pred = mejor_mlp.predict(X_nuevos)

# Reporte de clasificación en los datos no vistos
print("Reporte de clasificación en el conjunto de datos no vistos:")
print(classification_report(y_nuevos, y_nuevos_pred))